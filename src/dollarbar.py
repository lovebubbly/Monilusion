import pandas as pd
import numpy as np
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
from typing import List, Tuple, Optional, Dict
import os
import multiprocessing as mp # multiprocessing ì¶”ê°€
import numba # numba ì¶”ê°€

# --------------------------------------------------------------------------
# Helper í•¨ìˆ˜ë“¤ (ê¸°ì¡´ê³¼ ë™ì¼)
# --------------------------------------------------------------------------

def detect_column_mappings(df: pd.DataFrame) -> Dict[str, str]:
    """
    ğŸ” ë°ì´í„°í”„ë ˆì„ì˜ ì»¬ëŸ¼ëª…ì„ ìë™ ê°ì§€í•˜ê³  ë§¤í•‘í•©ë‹ˆë‹¤.
    """
    column_mapping = {
        'timestamp': None,
        'price': None,
        'volume': None
    }
    columns = df.columns.tolist()

    timestamp_candidates = ['timestamp', 'time', 'datetime', 'date', 'ts', 'trade_time']
    for col in columns:
        if any(candidate in col.lower() for candidate in timestamp_candidates):
            column_mapping['timestamp'] = col
            break
    
    price_candidates = ['price', 'close', 'last_price', 'px']
    for col in columns:
        if any(candidate in col.lower() for candidate in price_candidates):
            column_mapping['price'] = col
            break
    
    volume_candidates = ['volume', 'vol', 'qty', 'quantity', 'size', 'amount', 'base_volume']
    for col in columns:
        if any(candidate in col.lower() for candidate in volume_candidates):
            column_mapping['volume'] = col
            break
    
    return column_mapping

# --------------------------------------------------------------------------
# âœ¨ ìµœì í™”ëœ í•µì‹¬ í•¨ìˆ˜ë“¤ âœ¨
# --------------------------------------------------------------------------

def load_and_prepare_chunk_for_worker(chunk_files: List[str], column_mapping: Dict[str, str]) -> Optional[pd.DataFrame]:
    """
    ğŸ”¥ [Worker Processìš©] ThreadPoolExecutorë¥¼ ì‚¬ìš©í•œ ì•ˆì „í•œ íŒŒì¼ ë¡œë”© ë° ê¸°ë³¸ ì „ì²˜ë¦¬
    - ì´ í•¨ìˆ˜ëŠ” ê° ë³‘ë ¬ í”„ë¡œì„¸ìŠ¤ ë‚´ì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤.
    """
    dfs = []
    # ê° ì›Œì»¤ ë‚´ì—ì„œëŠ” I/O ì‘ì—…ì— íš¨ìœ¨ì ì¸ ìŠ¤ë ˆë“œ í’€ ì‚¬ìš©
    with ThreadPoolExecutor(max_workers=4) as executor:
        required_cols = [col for col in column_mapping.values() if col is not None]
        futures = {executor.submit(pd.read_parquet, f, columns=required_cols): f for f in chunk_files}
        for future in as_completed(futures):
            try:
                result = future.result()
                if result is not None and not result.empty:
                    dfs.append(result)
            except Exception:
                pass # ì—ëŸ¬ëŠ” ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œ ê°ì§€
    
    if not dfs:
        return None

    combined_df = pd.concat(dfs, ignore_index=True)
    
    ts_col, price_col, vol_col = column_mapping['timestamp'], column_mapping['price'], column_mapping['volume']

    if not all(col in combined_df.columns for col in [ts_col, price_col, vol_col]):
        return None
        
    combined_df[ts_col] = pd.to_datetime(combined_df[ts_col], unit='ms')
    combined_df[price_col] = pd.to_numeric(combined_df[price_col], errors='coerce')
    combined_df[vol_col] = pd.to_numeric(combined_df[vol_col], errors='coerce')
    combined_df.dropna(subset=[price_col, vol_col], inplace=True)
    
    combined_df['dollar_volume'] = combined_df[price_col] * combined_df[vol_col]
    
    return combined_df

def process_chunk_for_volume_analysis(args: Tuple[List[str], Dict[str, str]]) -> Optional[pd.Series]:
    """
    ğŸ”¥ [Worker Processìš©] íŒŒì¼ ì²­í¬ë¥¼ ë°›ì•„ ì¼ë³„ ê±°ë˜ëŒ€ê¸ˆìœ¼ë¡œ ë¦¬ìƒ˜í”Œë§í•˜ëŠ” í•¨ìˆ˜
    """
    chunk_files, column_mapping = args
    prepared_df = load_and_prepare_chunk_for_worker(chunk_files, column_mapping)
    if prepared_df is not None and not prepared_df.empty:
        ts_col = column_mapping['timestamp']
        return prepared_df.set_index(ts_col)['dollar_volume'].resample('D').sum()
    return None

def analyze_daily_dollar_volume_parallel(all_files: List[str]) -> Optional[pd.Series]:
    """
    ğŸš€ [ë©”ëª¨ë¦¬ ìµœì í™”] Multiprocessingì„ ì‚¬ìš©í•˜ì—¬ ì „ì²´ íŒŒì¼ì˜ ì¼ë³„ ì´ ê±°ë˜ëŒ€ê¸ˆì„ ë³‘ë ¬ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.
    """
    tqdm.write("Step 1: ì¼ë³„ ê±°ë˜ëŒ€ê¸ˆ ë³‘ë ¬ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    try:
        sample_df = pd.read_parquet(all_files[0])
        column_mapping = detect_column_mappings(sample_df)
        if not all(col for col in column_mapping.values()):
            raise ValueError("í•„ìˆ˜ ì»¬ëŸ¼(timestamp, price, volume)ì„ ìë™ ê°ì§€í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        tqdm.write(f"ğŸ¯ ì»¬ëŸ¼ ë§¤í•‘ ì„±ê³µ: {column_mapping}")
    except Exception as e:
        tqdm.write(f"âŒ ì´ˆê¸° ì»¬ëŸ¼ ë§¤í•‘ ì‹¤íŒ¨: {e}")
        return None

    num_cores = mp.cpu_count()
    tqdm.write(f"âœ… ì‚¬ìš© ê°€ëŠ¥ CPU ì½”ì–´: {num_cores}ê°œ. ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")

    num_chunks = min(len(all_files), num_cores * 4) 
    chunk_size = (len(all_files) + num_chunks - 1) // num_chunks
    file_chunks = [all_files[i:i + chunk_size] for i in range(0, len(all_files), chunk_size)]
    
    # [ë©”ëª¨ë¦¬ ìµœì í™”] ê²°ê³¼ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ëª¨ë‘ ìŒ“ì§€ ì•Šê³ , ì ì§„ì ìœ¼ë¡œ í•©ì‚°í•©ë‹ˆë‹¤.
    total_daily_volume = pd.Series(dtype=np.float64)
    
    with mp.Pool(processes=num_cores) as pool:
        tasks = [(chunk, column_mapping) for chunk in file_chunks]
        
        with tqdm(total=len(tasks), desc="ğŸ“Š ë³‘ë ¬ë¡œ ê±°ë˜ëŒ€ê¸ˆ ê³„ì‚° ì¤‘") as pbar:
            for result in pool.imap_unordered(process_chunk_for_volume_analysis, tasks):
                if result is not None and not result.empty:
                    # [ë©”ëª¨ë¦¬ ìµœì í™”] ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ê²°ê³¼ í•©ì‚° (ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëŒ€í­ ê°ì†Œ)
                    total_daily_volume = total_daily_volume.add(result, fill_value=0)
                pbar.update(1)

    if total_daily_volume.empty:
        tqdm.write("âš ï¸ ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
        
    tqdm.write("ğŸ”„ ë³‘ë ¬ ì²˜ë¦¬ ê²°ê³¼ í•©ì‚° ì™„ë£Œ!")
    return total_daily_volume.sort_index()

# --------------------------------------------------------------------------
# ì¶”ì²œ ë° ì‹œê°í™” í•¨ìˆ˜ (ê¸°ì¡´ê³¼ ë™ì¼)
# --------------------------------------------------------------------------

def get_threshold_recommendations(daily_dollar_volume: pd.Series, target_bars_per_day: int) -> Tuple[float, float, float]:
    """
    ğŸ¯ ë¶„ì„ëœ ì¼ë³„ ê±°ë˜ëŒ€ê¸ˆê³¼ ëª©í‘œ ë°” ê°œìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì  ì„ê³„ê°’ì„ ì¶”ì²œí•©ë‹ˆë‹¤.
    """
    tqdm.write("\nStep 2 & 3: ìµœì  ì„ê³„ê°’ ê³„ì‚°ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    full_period_avg = daily_dollar_volume.mean()
    bullish_market_avg = daily_dollar_volume.quantile(0.75)
    bearish_market_avg = daily_dollar_volume.quantile(0.25)
    
    recommended_threshold = full_period_avg / target_bars_per_day if target_bars_per_day > 0 else 0
    bullish_threshold = bullish_market_avg / target_bars_per_day if target_bars_per_day > 0 else 0
    bearish_threshold = bearish_market_avg / target_bars_per_day if target_bars_per_day > 0 else 0
    
    return recommended_threshold, bullish_threshold, bearish_threshold

def plot_volume_analysis(daily_dollar_volume: pd.Series, recommended_threshold: float, target_bars_per_day: int):
    """
    ğŸ¨ ë¶„ì„ ê²°ê³¼ë¥¼ ì‹œê°í™”í•˜ì—¬ ë³´ì—¬ì¤ë‹ˆë‹¤.
    """
    sns.set_theme(style="whitegrid", palette="viridis")
    fig, ax1 = plt.subplots(figsize=(16, 8))
    
    ax1.set_title(f'ì¼ë³„ ê±°ë˜ëŒ€ê¸ˆ ë¶„ì„ ë° ì¶”ì²œ ì„ê³„ê°’ (í•˜ë£¨ ëª©í‘œ ë°”: {target_bars_per_day}ê°œ)', fontsize=18, pad=20, weight='bold')
    ax1.plot(daily_dollar_volume.index, daily_dollar_volume, label='ì¼ë³„ ê±°ë˜ëŒ€ê¸ˆ', color='deepskyblue', alpha=0.7, zorder=2)
    ax1.set_xlabel('ë‚ ì§œ', fontsize=12)
    ax1.set_ylabel('ê±°ë˜ëŒ€ê¸ˆ (USD)', fontsize=12, color='deepskyblue')
    ax1.tick_params(axis='y', labelcolor='deepskyblue')
    ax1.grid(True, which='both', linestyle='--', linewidth=0.5)
    ax1.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x/1e9:.1f}B'))

    rolling_avg = daily_dollar_volume.rolling(window=30).mean()
    ax1.plot(rolling_avg.index, rolling_avg, color='mediumblue', linestyle='--', label='30ì¼ ì´ë™í‰ê· ', zorder=3)
    ax1.legend(loc='upper left')

    ax2 = ax1.twinx()
    expected_bars = daily_dollar_volume / recommended_threshold if recommended_threshold > 0 else pd.Series(0, index=daily_dollar_volume.index)
    ax2.plot(expected_bars.index, expected_bars, label='ì˜ˆìƒ ì¼ì¼ ë°” ê°œìˆ˜', color='darkorange', alpha=0.6, linestyle=':', zorder=1)
    ax2.set_ylabel('ì˜ˆìƒ ë°” ê°œìˆ˜ (ê°œ)', fontsize=12, color='darkorange')
    ax2.tick_params(axis='y', labelcolor='darkorange')
    ax2.axhline(y=target_bars_per_day, color='red', linestyle='--', label=f'ëª©í‘œ ë°” ê°œìˆ˜ ({target_bars_per_day}ê°œ)', zorder=4)
    ax2.legend(loc='upper right')

    fig.tight_layout(pad=1.5)
    plt.show()

# --------------------------------------------------------------------------
# ğŸš€ ë©”ì¸ ì‹¤í–‰ë¶€
# --------------------------------------------------------------------------

if __name__ == "__main__":
    # Windowsì—ì„œ multiprocessing ì‚¬ìš© ì‹œ í•„ìˆ˜
    if os.name == 'nt':
        mp.freeze_support()
    
    # --- 1. ì„¤ì • ---
    DATA_FOLDER = Path("C:/Monilusion/daily_cache")
    TARGET_BARS_PER_DAY = 100 
    
    # --- 2. íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ---
    all_files = sorted([str(f) for f in DATA_FOLDER.glob("*.parquet")])
    if not all_files:
        print(f"âŒ '{DATA_FOLDER}' ê²½ë¡œì— parquet íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    else:
        print(f"âœ… ì´ {len(all_files)}ê°œì˜ ë°ì´í„° íŒŒì¼ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
        
        # --- 3. ìµœì í™”ëœ ë¶„ì„ ì‹¤í–‰ ---
        daily_volume_df = analyze_daily_dollar_volume_parallel(all_files)
        
        if daily_volume_df is not None and not daily_volume_df.empty:
            
            # --- 4. ì„ê³„ê°’ ì¶”ì²œ ---
            rec_thresh, bull_thresh, bear_thresh = get_threshold_recommendations(daily_volume_df, TARGET_BARS_PER_DAY)
            
            # --- 5. ê²°ê³¼ ì¶œë ¥ ë° ì‹œê°í™” ---
            print("\n" + "="*50)
            print("ï¿½ ë‹¬ëŸ¬ ë°” ì„ê³„ê°’ ë³‘ë ¬ ë¶„ì„ ê²°ê³¼ (ë©”ëª¨ë¦¬ ìµœì í™”) ğŸš€")
            print("="*50)
            print(f"ğŸ¯ í•˜ë£¨ í‰ê·  ëª©í‘œ ë°” ê°œìˆ˜: {TARGET_BARS_PER_DAY} ê°œ")
            print(f"ğŸ’° ì „ì²´ ê¸°ê°„ ì¼í‰ê·  ê±°ë˜ëŒ€ê¸ˆ: ${daily_volume_df.mean():,.0f}")
            print("\n--- ì¶”ì²œ ì„ê³„ê°’ ---")
            print(f"â­ ê· í˜• ì„ê³„ê°’ (ì „ì²´ ê¸°ê°„ í‰ê·  ê¸°ë°˜): ${rec_thresh:,.0f}")
            print(f"ğŸ”¥ í™œí™©ì¥ ì„ê³„ê°’ (ìƒìœ„ 25% ê¸°ê°„ ê¸°ë°˜): ${bull_thresh:,.0f}")
            print(f"â„ï¸ ì¹¨ì²´ì¥ ì„ê³„ê°’ (í•˜ìœ„ 25% ê¸°ê°„ ê¸°ë°˜): ${bear_thresh:,.0f}")
            print("="*50)
            print("\nğŸ’¡ ì œì•ˆ:")
            print(f"1. ì‹œì‘ì : ìš°ì„  'ê· í˜• ì„ê³„ê°’'ì¸ ${rec_thresh:,.0f}ì„(ë¥¼) ë„ˆì˜ ë‹¬ëŸ¬ ë°” ìƒì„± ìŠ¤í¬ë¦½íŠ¸ì— ì ìš©í•´ë´.")
            print("2. ì „ëµ ì„¸ë¶„í™”: ë§Œì•½ ì‹œì¥ êµ­ë©´(í™œí™©/ì¹¨ì²´)ì„ ë¯¸ë¦¬ íŒë‹¨í•  ìˆ˜ ìˆë‹¤ë©´, í•´ë‹¹ êµ­ë©´ì— ë§ëŠ” ì„ê³„ê°’ì„ ì‚¬ìš©í•˜ëŠ” ë™ì  ì „ëµë„ ê³ ë ¤í•´ë³¼ ìˆ˜ ìˆì–´.")
            print("3. ì‹œê°í™” í™•ì¸: ì•„ë˜ ì°¨íŠ¸ë¥¼ ë³´ê³  ì¶”ì²œ ì„ê³„ê°’ ì ìš© ì‹œ ì¼ë³„ ë°” ê°œìˆ˜ê°€ ì–´ë–»ê²Œ ë³€ë™í•˜ëŠ”ì§€ í™•ì¸í•´ë´. (ëª©í‘œì„ ì— ê·¼ì ‘í•˜ëŠ”ì§€)")
            
            plot_volume_analysis(daily_volume_df, rec_thresh, TARGET_BARS_PER_DAY)